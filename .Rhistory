crpus<-tm_map(crpus,stripWhitespace)
crpus<-tm_map(crpus,removeWords,stopwords("en"))
crpus<-tm_map(crpus,PlainTextDocument)
# using the ngram package, we'll define a function similar to the RWeka package but
# using the ngram package
explore_ngram<-function(x) ngram_asweka(x,min=2, max=2)
dtm<-DocumentTermMatrix(crpus,control=list(tokenize=explore_ngram))
freq<-colSums(as.matrix(dtm))
length(freq)
crpus[1]
crpus[1][1]
as.character(crpus)[12:50]
library(tm)
library(ngram)
cname<-file.path("C:/Users/gmaerina/Documents/Coursera/Capstone","training")
# during exploratory analysis, it was found that extraneous characters revealed themselves that
# caused issues in determining a viable set of tokenized words in the corpus
# it seems that the file was saved from web page in some other format that when read into
# R caused unxpected characters to show themselves
crpus<-Corpus(DirSource(cname,encoding="UTF8))
crpus<-tm_map(crpus,removeNumbers)
# use the built in processing functions of the tm package
crpus<-tm_map(crpus,removePunctuation)
crpus<-tm_map(crpus,tolower)
crpus<-tm_map(crpus,stripWhitespace)
crpus<-tm_map(crpus,removeWords,stopwords("en"))
crpus<-tm_map(crpus,PlainTextDocument)
# using the ngram package, we'll define a function similar to the RWeka package but
# using the ngram package
explore_ngram<-function(x) ngram_asweka(as.character(x),min=2, max=2)
dtm<-DocumentTermMatrix(crpus,control=list(tokenize=explore_ngram))
freq<-colSums(as.matrix(dtm))
length(freq)
library(tm)
library(ngram)
cname<-file.path("C:/Users/gmaerina/Documents/Coursera/Capstone","training")
# during exploratory analysis, it was found that extraneous characters revealed themselves that
# caused issues in determining a viable set of tokenized words in the corpus
# it seems that the file was saved from web page in some other format that when read into
# R caused unxpected characters to show themselves
crpus<-Corpus(DirSource(cname,encoding="UTF-8"))
crpus<-tm_map(crpus,removeNumbers)
# use the built in processing functions of the tm package
crpus<-tm_map(crpus,removePunctuation)
crpus<-tm_map(crpus,tolower)
crpus<-tm_map(crpus,stripWhitespace)
crpus<-tm_map(crpus,removeWords,stopwords("en"))
crpus<-tm_map(crpus,PlainTextDocument)
# using the ngram package, we'll define a function similar to the RWeka package but
# using the ngram package
explore_ngram<-function(x) ngram_asweka(as.character(x),min=2, max=2)
dtm<-DocumentTermMatrix(crpus,control=list(tokenize=explore_ngram))
freq<-colSums(as.matrix(dtm))
length(freq)
cname<-file.path("C:/Users/gmaerina/Documents/Coursera/Capstone/SwiftKey/Coursera-SwiftKey/final","test")
library(tm)
# during exploratory analysis, it was found that extraneous characters revealed themselves that
# caused issues in determining a viable set of tokenized words in the corpus
# it seems that the file was saved from web page in some other format that when read into
# R caused unxpected characters to show themselves
crpus<-Corpus(DirSource(cname,encoding="UTF-8"))
crpus<-tm_map(crpus,removeNumbers)
# need additional cleanup of corpus
# there are other character that need to be discounted from processing
for( j in seq(crpus))
{
crpus[[j]]<-gsub("-"," ",crpus[[j]])
crpus[[j]]<-gsub("\\"," ",crpus[[j]])
}
# use the built in processing functions of the tm package
crpus<-tm_map(crpus,tolower)
crpus<-tm_map(crpus,removePunctuation)
crpus<-tm_map(crpus,stripWhitespace)
crpus<-tm_map(crpus,removeWords,stopwords("en"))
crpus<-tm_map(crpus,PlainTextDocument)
# using the ngram package, we'll define a function similar to the RWeka package but
# using the ngram package
some_function<-function(x) ngram_asweka(as.character(x),min=2, max=4)
dtm<-DocumentTermMatrix(crpus,control=list(tokenize=some_function))
cname<-file.path("C:/Users/gmaerina/Documents/Coursera/Capstone","training")
library(tm)
# during exploratory analysis, it was found that extraneous characters revealed themselves that
# caused issues in determining a viable set of tokenized words in the corpus
# it seems that the file was saved from web page in some other format that when read into
# R caused unxpected characters to show themselves
crpus<-Corpus(DirSource(cname,encoding="UTF-8"))
crpus<-tm_map(crpus,removeNumbers)
# need additional cleanup of corpus
# there are other character that need to be discounted from processing
for( j in seq(crpus))
{
crpus[[j]]<-gsub("-"," ",crpus[[j]])
crpus[[j]]<-gsub("\\"," ",crpus[[j]])
}
# use the built in processing functions of the tm package
crpus<-tm_map(crpus,tolower)
crpus<-tm_map(crpus,removePunctuation)
crpus<-tm_map(crpus,stripWhitespace)
crpus<-tm_map(crpus,removeWords,stopwords("en"))
crpus<-tm_map(crpus,PlainTextDocument)
# using the ngram package, we'll define a function similar to the RWeka package but
# using the ngram package
some_function<-function(x) ngram_asweka(as.character(x),min=2, max=4)
dtm<-DocumentTermMatrix(crpus,control=list(tokenize=some_function))
#dtm<-DocumentTermMatrix(crpus)
freq<-colSums(as.matrix(dtm))
head(freq)
install.packages(ngram)
install.packages("ngram")
install.packages("ngram")
library(tm)
library(ngram)
cname<-file.path("C:/Users/gmaerina/Documents/Coursera/Capstone","training")
# during exploratory analysis, it was found that extraneous characters revealed themselves that
# caused issues in determining a viable set of tokenized words in the corpus
# it seems that the file was saved from web page in some other format that when read into
# R caused unxpected characters to show themselves
crpus<-Corpus(DirSource(cname,encoding="UTF-8"))
crpus<-tm_map(crpus,removeNumbers)
# use the built in processing functions of the tm package
crpus<-tm_map(crpus,removePunctuation)
crpus<-tm_map(crpus,tolower)
crpus<-tm_map(crpus,stripWhitespace)
crpus<-tm_map(crpus,removeWords,stopwords("en"))
crpus<-tm_map(crpus,PlainTextDocument)
# using the ngram package, we'll define a function similar to the RWeka package but
# using the ngram package
explore_ngram<-function(x) ngram_asweka(as.character(x),min=2, max=2)
dtm<-DocumentTermMatrix(crpus,control=list(tokenize=explore_ngram))
freq<-colSums(as.matrix(dtm))
length(freq)
library(ngram)
library(tm)
library(ngram)
cname<-file.path("C:/Users/gmaerina/Documents/Coursera/Capstone","training")
# during exploratory analysis, it was found that extraneous characters revealed themselves that
# caused issues in determining a viable set of tokenized words in the corpus
# it seems that the file was saved from web page in some other format that when read into
# R caused unxpected characters to show themselves
crpus<-Corpus(DirSource(cname,encoding="UTF-8"))
crpus<-tm_map(crpus,removeNumbers)
for(i in seq(crpus))
{
crpus[[i]]<-gsub("[^a-zA-Z]+", " ",crpus[[i]])
}
# use the built in processing functions of the tm package
crpus<-tm_map(crpus,removePunctuation)
crpus<-tm_map(crpus,tolower)
crpus<-tm_map(crpus,stripWhitespace)
crpus<-tm_map(crpus,removeWords,stopwords("en"))
crpus<-tm_map(crpus,PlainTextDocument)
# using the ngram package, we'll define a function similar to the RWeka package but
# using the ngram package
explore_ngram<-function(x) ngram_asweka(as.character(x),min=2, max=4)
dtm<-DocumentTermMatrix(crpus,control=list(tokenize=explore_ngram))
freq<-colSums(as.matrix(dtm))
length(freq)
freq
library(tm)
library(ngram)
cname<-file.path("C:/Users/gmaerina/Documents/Coursera/Capstone","training")
# during exploratory analysis, it was found that extraneous characters revealed themselves that
# caused issues in determining a viable set of tokenized words in the corpus
# it seems that the file was saved from web page in some other format that when read into
# R caused unxpected characters to show themselves
crpus<-Corpus(DirSource(cname,encoding="UTF-8"))
crpus<-tm_map(crpus,removeNumbers)
for(i in seq(crpus))
{
crpus[[i]]<-gsub("[^a-zA-Z]+", "",crpus[[i]])
}
# use the built in processing functions of the tm package
crpus<-tm_map(crpus,removePunctuation)
crpus<-tm_map(crpus,tolower)
crpus<-tm_map(crpus,stripWhitespace)
crpus<-tm_map(crpus,removeWords,stopwords("en"))
crpus<-tm_map(crpus,PlainTextDocument)
# using the ngram package, we'll define a function similar to the RWeka package but
# using the ngram package
explore_ngram<-function(x) ngram_asweka(as.character(x),min=2, max=4)
dtm<-DocumentTermMatrix(crpus,control=list(tokenize=explore_ngram))
freq<-colSums(as.matrix(dtm))
length(freq)
library(tm)
library(ngram)
cname<-file.path("C:/Users/gmaerina/Documents/Coursera/Capstone","training")
# during exploratory analysis, it was found that extraneous characters revealed themselves that
# caused issues in determining a viable set of tokenized words in the corpus
# it seems that the file was saved from web page in some other format that when read into
# R caused unxpected characters to show themselves
crpus<-Corpus(DirSource(cname,encoding="UTF-8"))
crpus<-tm_map(crpus,removeNumbers)
for(i in seq(crpus))
{
crpus[[i]]<-gsub("\\", "",crpus[[i]])
}
# use the built in processing functions of the tm package
crpus<-tm_map(crpus,removePunctuation)
crpus<-tm_map(crpus,tolower)
crpus<-tm_map(crpus,stripWhitespace)
crpus<-tm_map(crpus,removeWords,stopwords("english"))
crpus<-tm_map(crpus,PlainTextDocument)
# using the ngram package, we'll define a function similar to the RWeka package but
# using the ngram package
explore_ngram<-function(x) ngram_asweka(as.character(x),min=2, max=4)
dtm<-DocumentTermMatrix(crpus,control=list(tokenize=explore_ngram))
freq<-colSums(as.matrix(dtm))
length(freq)
install.packages("RWeka")
library(tm)
library(ngram)
library(RWeka)
cname<-file.path("C:/Users/gmaerina/Documents/Coursera/Capstone","training")
# during exploratory analysis, it was found that extraneous characters revealed themselves that
# caused issues in determining a viable set of tokenized words in the corpus
# it seems that the file was saved from web page in some other format that when read into
# R caused unxpected characters to show themselves
crpus<-Corpus(DirSource(cname,encoding="UTF-8"))
crpus<-tm_map(crpus,removeNumbers)
for(i in seq(crpus))
{
crpus[[i]]<-gsub("\\", "",crpus[[i]])
}
# use the built in processing functions of the tm package
crpus<-tm_map(crpus,removePunctuation)
crpus<-tm_map(crpus,tolower)
crpus<-tm_map(crpus,stripWhitespace)
crpus<-tm_map(crpus,removeWords,stopwords("english"))
crpus<-tm_map(crpus,PlainTextDocument)
# using the ngram package, we'll define a function similar to the RWeka package but
# using the ngram package
explore_ngram<-function(x) ngram_asweka(as.character(x),min=2, max=4)
explore_bweka<-function(x) NGramTokenizer(x,Weka_control(min=2,max=2))
dtm<-DocumentTermMatrix(crpus,control=list(tokenize=explore_bweka))
freq<-colSums(as.matrix(dtm))
length(freq)
library(RWeka)
library("RWeka")
library("RWekaJars")
library(RWeka)
install.packages("rJava")
library(RWeka)
library('RWeka')
library('rJava')
library(tm)
library(RWeka)
install.packages("RWeka")
library(RWeka)
install.packages("RWeka", dependencies = TRUE)
install.packages("rJava", dependencies = TRUE)
library(RWeka)
library(tm)
library(ngram)
library(RWeka)
cname<-file.path("C:/Users/gmaerina/Documents/Coursera/Capstone","training")
# during exploratory analysis, it was found that extraneous characters revealed themselves that
# caused issues in determining a viable set of tokenized words in the corpus
# it seems that the file was saved from web page in some other format that when read into
# R caused unxpected characters to show themselves
crpus<-Corpus(DirSource(cname,encoding="UTF-8"))
crpus<-tm_map(crpus,removeNumbers)
crpus<-tm_map(crpus,removePunctuation)
for(i in seq(crpus))
{
crpus[[i]]<-gsub("\\", " ",crpus[[i]])
}
# use the built in processing functions of the tm package
crpus<-tm_map(crpus,tolower)
crpus<-tm_map(crpus,stripWhitespace)
#crpus<-tm_map(crpus,removeWords,stopwords("en"))
crpus<-tm_map(crpus,PlainTextDocument)
# using the ngram package, we'll define a function similar to the RWeka package but
# using the ngram package
some_function<-function(x) ngram_asweka(as.character(x),min=3, max=3,sep=" ")
explore_bweka<-function(x) NGramTokenizer(x,Weka_control(min=3,max=3))
dtm<-DocumentTermMatrix(crpus,control=list(tokenize=some_function))
freq<-colSums(as.matrix(dtm))
length(freq)
hist(freq)
l<-some_function(crpus)
tdm<-as.TermDocumentMatrix(l)
tdm<-as.TermDocumentMatrix(l,weighting=NULL)
tdm<-as.TermDocumentMatrix(l,weighting=WeightFunction(function(m)m>0,m,NULL))
tdm<-as.TermDocumentMatrix(l,weighting=WeightFunction(function(m)m>0,m,NULL))
tdm<-as.TermDocumentMatrix(l,weighting=WeightFunction(function(m) m > 0, m))
tdm<-as.TermDocumentMatrix(l)
tdm<-as.TermDocumentMatrix(l,weighting="weightTf")
tdm<-as.TermDocumentMatrix(l,weighting="weightSMART")
some_function<-function(x) ngram_asweka(as.character(x),min=2, max=2,sep=" ")
tdm<-as.TermDocumentMatrix(crpus, list(tokenize=some_function))
tdm<-TermDocumentMatrix(crpus, list(tokenize=some_function))
tdm<-TermDocumentMatrix(crpus, control=list(tokenize=some_function))
help(list)
tdm<-TermDocumentMatrix(crpus, control=list(tokenizer=some_function))
dtm<-DocumentTermMatrix(crpus, control=list(tokenizer=some_function))
dtm<-DocumentTermMatrix(crpus, control=list(tokenizer=some_function))
DocumentTermMatrix(crpus, control=list(tokenizer=some_function))
DocumentTermMatrix(crpus, control=list(tokenizer=some_function))
some_function<-function(x) ngram_asweka(as.character(x),min=2, max=4,sep=" ")
DocumentTermMatrix(crpus, control=list(tokenizer=some_function))
some_function<-function(x) ngram_asweka(x,min=2, max=4,sep=" ")
DocumentTermMatrix(as.character(crpus), control=list(tokenizer=some_function))
library(tm)
#library(RWeka)
cname<-file.path("C:/Users/gmaerina/Documents/Coursera/Capstone","training")
# during exploratory analysis, it was found that extraneous characters revealed themselves that
# caused issues in determining a viable set of tokenized words in the corpus
# it seems that the file was saved from web page in some other format that when read into
# R caused unxpected characters to show themselves
crpus<-Corpus(DirSource(cname,encoding="UTF-8"))
crpus<-tm_map(crpus,removeNumbers)
crpus<-tm_map(crpus,removePunctuation)
for(i in seq(crpus))
{
crpus[[i]]<-gsub("\\", " ",crpus[[i]])
}
# use the built in processing functions of the tm package
crpus<-tm_map(crpus,tolower)
crpus<-tm_map(crpus,stripWhitespace)
#crpus<-tm_map(crpus,removeWords,stopwords("en"))
crpus<-tm_map(crpus,PlainTextDocument)
# using the ngram package, we'll define a function similar to the RWeka package but
# using the ngram package
library(ngram)
some_function<-function(x) ngram_asweka(as.character(x),min=3, max=3,sep=" ")
#explore_bweka<-function(x) NGramTokenizer(x,Weka_control(min=3,max=3))
dtm<-DocumentTermMatrix(crpus,control=list(tokenize=some_function))
freq<-colSums(as.matrix(dtm))
length(freq)
hist(freq)
library(RWeka)
library(RWeka)
library(rJava)
library(RWeka)
library(RWeka)
library("RWeka")
sys.which(javac)
Sys.which(javac)
library(RWeka)
Sys.which(javac)
Sys.which(javac)
Sys.which(javac)
Sys.which(javac)
Sys.which(java)
Sys.which("java")
Sys.which("javac")
library(RWeka)
install.packages(rJava)
install.packages("rJava")
install.packages("RWeka")
library(RWeka)
Sys.which("java")
Sys.which("javac")
library(reshape2)
help(hist)
library(tm)
library(ngram)
cname<-file.path("C:/Users/gmaerina/Documents/Coursera/Capstone/SwiftKey/Coursera-SwiftKey/final","training")
# during exploratory analysis, it was found that extraneous characters revealed themselves that
# caused issues in determining a viable set of tokenized words in the corpus
# it seems that the file was saved from web page in some other format that when read into
# R caused unxpected characters to show themselves
crpus<-Corpus(DirSource(cname,encoding="UTF-8"))
crpus<-tm_map(crpus,removeNumbers)
# need additional cleanup of corpus
# there are other character that need to be discounted from processing
for( j in seq(crpus))
{
crpus[[j]]<-gsub("-"," ",crpus[[j]])
crpus[[j]]<-gsub("\\"," ",crpus[[j]])
}
# use the built in processing functions of the tm package
crpus<-tm_map(crpus,removePunctuation)
crpus<-tm_map(crpus,tolower)
crpus<-tm_map(crpus,stripWhitespace)
crpus<-tm_map(crpus,removeWords,stopwords("english"))
crpus<-tm_map(crpus,PlainTextDocument)
# using the ngram package, we'll define a function similar to the RWeka package but
# using the ngram package
explore_ngram<-function(x) ngram_asweka(as.character(x),min=2, max=2)
dtm<-DocumentTermMatrix(crpus,control=list(tokenize=explore_ngram))
dakine<-colSums(as.matrix(dtm))
length(dakine)
cname<-file.path("C:/Users/gmaerina/Documents/Coursera/Capstone","training")
During exploration, the package, ngram, was used for ngram development. The choice of using ngram was because it was built on C, elevating my expectatioins to produce an application with higher performace.  It was later found that on my host platform, implementing a tokenizer build on ngram+tm produced results inconsistent with the standalone operation of the ngram function. It was decided to switch to the RWeka package and implement a tokenizer using RWeka+tm.
Sys.which("java")
options(java.home="C:\\WINDOWS\\SYSTEM32\\java.exe")
Sys.which("java")
options(java.home="C:\\WINDOWS\\SYSTEM32\\")
Sys.which("java")
options
options()
library(tm)
library(RWeka)
remove.packages("rJAva", "RWeka")
remove.packages("rJava", "RWeka")
install.packages("RWeka", dependencies = TRUE)
library(tm)
library('RWeka')
Sys.setenv(JAVA_HOME="")
library/(RWeka)
library(RWeka)
Sys.setenv(JAVA_HOME="")
Sys.setenv(JAVA_HOME="")
#
# 2016/06/23  gmaerina  Noticed that RStudio could no longer load the RWeka library
#                       with warning messages pointing to not being able to either find the
#                       rJava module nor a java.dll.
#                       Remedy is to uninsall RWeka and rJava and do a complete reinstall
#                       using the repository from Singapore. Other repositories do not have
#                       the necessary dependencies
#                       Another solution is to unset the JAVA_HOME
#                       environment variable before calling RWeka
#                       >Sys.setenv(JAVA_HOME="")
#
##
## unset JAVA_HOME
##
Sys.setenv(JAVA_HOME="")
# Be sure to take into consideration the size of the files when
# processing on the stack. Load the java option before loadng any
# component that depends on java
# -Xmx for heap and -Xms for stack
options(java.parameters = "-Xmx2g")
#options(java.parameters = "-Xms2g")
library(tm)
library(RWeka)
cname<-file.path("C:/Users/gmaerina/Documents/Coursera/Capstone","training")
# during exploratory analysis, it was found that extraneous characters revealed themselves that
# caused issues in determining a viable set of tokenized words in the corpus
# it seems that the file was saved from web page in some other format that when read into
# R caused unxpected characters to show themselves
crpus<-Corpus(DirSource(cname,encoding="UTF-8"))
crpus<-tm_map(crpus,removeNumbers)
crpus<-tm_map(crpus,removePunctuation)
# use the built in processing functions of the tm package
crpus<-tm_map(crpus,tolower)
crpus<-tm_map(crpus,stripWhitespace)
crpus<-tm_map(crpus,removeWords,stopwords("english"))
crpus<-tm_map(crpus,PlainTextDocument)
explore_bweka<-function(x) NGramTokenizer(x,Weka_control(min=2,max=4))
dtm<-DocumentTermMatrix(crpus,control=list(tokenize=explore_bweka))
freq<-colSums(as.matrix(dtm))
length(freq)
hist(freq)
#
# At this point we have a list of values that we can use as a dictionary,
# we need to create a list of frequencies, that we can use as a pseduo
# lookup table that we can use for text prediction.
#
# The concept is that we will have a 3 lists, one for 2 ngrams, 3-ngrams, and
# 4-ngrams. For each list, we will compare n-word values to find a reasonable
# match.
Sys.setenv(JAVA_HOME="")
options(java.parameters = "-Xmx2g")
library(RWeka)
library(RWeka)
library(RWeka)
help(free)
help(clear)
help(clr)
library('RWeka')
options(java.home)
Sys.setenv(JAVA_HOME="")
library(RWeka)
options(java.home="C:\\Program Files\\Java\\jdk1.7.0_10\\jre\\bin")
library(RWeka)
q()
library(doParallel)
library(foreach)
pol<-c(1,2,3,4,5)
foreach(pol) %dopar%{
print("one")
}
cl<-makeCluster(4)
registerDoParallel(cl)
foreach(pol) %dopar%{
print("one")
}
foreach(pol) %dopar%{
Sys.time()
}
q()
library(devtools)
install_github('slidify','ramnathv')
install_github('slidify','ramnathv')
install_github('slidify')
install.packages('slidify')
library('devtools')
install.packages('slidify')
library('slidify')
author('testdeck')
slidify('testdeck')
install_github('ramnathv/slidify')
install_github('ramnathv/slidify')
install_github('ramnathv/slidify')
install_github('ramnathv/slidify')
install_github('ramnathv/slidify', force=TRUE)
install.packages('slidify', force=TRUE)
install.packages('slidify', force=TRUE)
library('slidify')
slidify('testdeck')
slidify('testdeck')
library('slidify')
library('ramnathv/slidify')
library('ramnathv')
library(slidifyLibraries)
library(slidify)
library(slidify)
library(slidify)
setwd("C:/Users/gmaerina/Documents/Coursera/Capstone/Slidify")
author("CapstoneProject")
